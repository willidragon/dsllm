\chapter{Related Work}

\section{Human Activity Recognition with Wearable Sensors}

\hspace{2em}Human Activity Recognition (HAR) using wearable sensors has emerged as a fundamental research area with applications spanning healthcare monitoring, smart homes, and fitness tracking. Early comprehensive surveys by \cite{Bulling2014HARSurvey} and \cite{Lara2013Mobile} established the foundational understanding of sensor-based activity recognition, highlighting the challenges of feature extraction from multi-modal sensor data and the importance of temporal modeling for sequential activities.

The evolution of HAR has been significantly influenced by advances in deep learning methodologies. Traditional approaches relied heavily on hand-crafted features and shallow machine learning models, but the introduction of deep neural networks revolutionized the field. \cite{Wang2019DeepHAR} provided a comprehensive survey demonstrating how deep learning techniques could automatically learn hierarchical representations from raw sensor data, eliminating the need for manual feature engineering. This paradigm shift enabled more robust and generalizable activity recognition systems.

\subsection{Deep Learning Approaches for HAR}

\hspace{2em}The application of deep learning to HAR has yielded remarkable improvements in recognition accuracy and model generalization. \cite{Yang2015DeepConvLSTM} pioneered the use of deep convolutional neural networks on multichannel time series data, demonstrating superior performance compared to traditional methods. Their approach effectively captured both spatial relationships between sensor channels and temporal dependencies within activity sequences.

Convolutional Neural Networks (CNNs) have proven particularly effective for sensor data processing. \cite{Ignatov2018RealTime} developed real-time HAR systems using CNNs on accelerometer data, achieving impressive accuracy while maintaining computational efficiency suitable for mobile deployment. Similarly, \cite{Ronao2016Human} explored smartphone-based HAR using deep learning, showcasing the potential for ubiquitous activity monitoring through everyday devices.

The integration of recurrent architectures has further enhanced temporal modeling capabilities. \cite{Hammerla2016Deep} conducted comprehensive comparisons of deep, convolutional, and recurrent models for wearable-based HAR, establishing best practices for different types of sensor configurations and activity categories. Their work highlighted the importance of model architecture selection based on the specific characteristics of the target application.

\section{Transformer and Attention Mechanisms in HAR}

\hspace{2em}The transformer architecture, originally introduced by \cite{Vaswani2017Attention}, has revolutionized sequence modeling across various domains. The self-attention mechanism's ability to capture long-range dependencies and parallel processing capabilities have made it particularly attractive for sensor data analysis. Recent works have successfully adapted transformer architectures for HAR applications, demonstrating significant improvements over traditional recurrent approaches.

\cite{Li2024P2LHAP} introduced P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles activity recognition, segmentation, and forecasting simultaneously. Their approach divides sensor data streams into patches served as input tokens, enabling unified processing of multiple HAR tasks within a single model. The patch-based tokenization strategy has proven effective for handling variable-length sensor sequences while maintaining computational efficiency.

Recent advances have also explored specialized transformer variants for sensor data. \cite{Zhang2025MoPFormer} proposed MoPFormer, a Motion-Primitive Transformer that enhances interpretability by tokenizing sensor signals into semantically meaningful motion primitives. This approach addresses the critical challenge of cross-dataset generalization by learning fundamental movement patterns that remain consistent across different data sources.

\cite{Zhao2025ActionFormer} adapted the ActionFormer architecture for sensor-based HAR, demonstrating substantial improvements in activity boundary detection and classification accuracy. Their work highlighted the importance of detecting informative channels in multi-sensor configurations, achieving significant performance gains through attention-based channel selection mechanisms.

\section{Large Language Models for Sensor Data}

\hspace{2em}The recent success of Large Language Models (LLMs) has inspired researchers to explore their application to sensor data understanding. The foundational work in this area includes BERT \cite{Devlin2019BERT} and GPT-3 \cite{Brown2020GPT3}, which demonstrated the power of large-scale pre-training for natural language understanding. These advances have motivated the development of similar approaches for sensor modalities.

\cite{li2024sensorllm} introduced SensorLLM, a pioneering work that aligns large language models with motion sensors for human activity recognition. Their two-stage framework consists of Sensor-Language Alignment for learning shared representations between sensor data and natural language, followed by Task-Aware Tuning for specific downstream applications. SensorLLM demonstrated remarkable capabilities in zero-shot activity recognition and cross-modal reasoning, establishing a new paradigm for sensor data understanding.

Building upon this foundation, \cite{Zhang2025SensorLM} developed SensorLM, a comprehensive sensor-language foundation model that processes over 59.7 million hours of data from more than 103,000 participants. Their hierarchical caption generation pipeline captures statistical, structural, and semantic information from sensor data, enabling sophisticated natural language interactions with wearable sensor systems.

\cite{Civitarese2024ADLLLM} explored the application of LLMs for Activities of Daily Living recognition in smart home environments. Their ADL-LLM system transforms raw sensor data into textual representations processed by LLMs for zero-shot recognition, demonstrating the potential for deployment in real-world assisted living scenarios.

% Newly added works on zero-shot LLM-based HAR and comprehensive surveys
\cite{Ji2024HARGPT} presented \textit{HARGPT}, a systematic study investigating whether off-the-shelf LLMs can serve as zero-shot human activity recognizers without any gradient-based fine-tuning. By feeding raw, downsampled (\SI{10}{\hertz}) IMU sequences directly into GPT-4 and other large models with carefully crafted role-play and chain-of-thought prompts, HARGPT achieved approximately 80\% accuracy on the Capture-24 and HHAR benchmarks. This evidence reveals an emergent ability of LLMs to transform noisy time–series signals into high-level activity semantics purely through prompting, albeit at the expense of large context windows and intensive prompt engineering. In contrast, our two-stage pipeline fine-tunes a compact Chronos encoder--LLaMA architecture on teacher--student enhanced, coarse-grained data, eliminating elaborate prompt design while enabling efficient on-device inference.

Complementing these empirical findings, \cite{Ferrara2024LLMSurvey} provides the first dedicated survey on LLM-driven wearable sensing. The paper synthesizes early trends, publicly available datasets, evaluation protocols, and open challenges—such as privacy, robustness, and energy efficiency—thereby situating the rapidly growing body of LLM-for-HAR research within a broader interdisciplinary landscape.

\subsection{High-Level Reasoning with LLMs}

\hspace{2em}Beyond basic activity classification, recent research has investigated LLMs' capabilities for high-level reasoning over sensor traces. \cite{Ouyang2024LLMSense} developed LLMSense, which harnesses LLMs for complex spatiotemporal reasoning tasks such as dementia diagnosis and occupancy tracking. Their framework demonstrates how LLMs can comprehend long-term sensor patterns and make sophisticated inferences that go beyond simple activity labeling.

\cite{Sun2024IoTActivity} presented an AI-based system utilizing IoT-enabled ambient sensors and LLMs for complex activity tracking in elderly care. Their approach combines edge device processing with cloud-based LLM reasoning, addressing privacy concerns while maintaining the sophisticated reasoning capabilities required for healthcare applications.

The integration of multiple sensor modalities has been explored by \cite{Mo2024IoTLM}, who introduced IoT-LM, a large multisensory language model for the Internet of Things. Their MultiIoT dataset encompasses over 1.15 million samples from 12 modalities, enabling comprehensive multisensory understanding and interactive question-answering capabilities.

\section{Energy Efficiency and Resource Constraints}

\hspace{2em}A critical challenge in HAR deployment is the balance between recognition accuracy and computational efficiency, particularly for battery-powered wearable devices. This challenge has motivated extensive research into lightweight model architectures and energy-efficient processing strategies.

Traditional approaches to address computational constraints include model compression techniques, quantization, and pruning. However, these methods often require careful tuning and may significantly impact model performance. The development of inherently efficient architectures has emerged as a more promising direction.

\cite{Ruan2024PAT} introduced the Pretrained Actigraphy Transformer (PAT), designed specifically for time-series wearable movement data with emphasis on computational efficiency. Their lightweight transformer architecture achieves state-of-the-art performance in mental health prediction tasks while maintaining interpretability and deployment feasibility on resource-constrained devices.

\subsection{Data Sampling and Downsampling Strategies}

\hspace{2em}The relationship between sensor data resolution and recognition performance presents a fundamental trade-off in wearable HAR systems. Higher sampling rates provide richer temporal information but significantly increase computational and energy costs. Conversely, lower sampling rates extend battery life but may compromise recognition accuracy.

This trade-off becomes particularly critical for continuous monitoring applications where devices must operate for extended periods without charging. The challenge of maintaining recognition performance with downsampled sensor data remains largely unexplored in the literature, representing a significant gap that our work addresses.

Current HAR systems typically assume high-frequency sensor data (50-100 Hz or higher), which may not be practical for real-world deployment scenarios where energy efficiency is paramount. The development of effective methods for coarse-grained sensor data processing is essential for enabling practical wearable HAR systems.

\section{Research Gaps and Motivation}

\hspace{2em}Despite significant advances in HAR research, several critical gaps remain. First, while transformer-based approaches have shown promise, their application to coarse-grained sensor data has received limited attention. Most existing work assumes high-resolution sensor inputs, which may not be feasible in energy-constrained environments.

Second, the integration of LLMs with sensor data has primarily focused on high-frequency, well-sampled data. The effectiveness of LLM-based approaches for downsampled sensor data remains unexplored, despite its practical importance for real-world deployment.

Third, current research lacks comprehensive analysis of the attention patterns and tokenization strategies specifically designed for low-precision sensor data. Understanding how transformer models process coarse-grained sensor information is crucial for developing effective architectures.

Finally, there is insufficient investigation into the fundamental limits of activity recognition with reduced sensor resolution. Establishing these limits and developing methods to approach them is essential for advancing the field toward practical, energy-efficient HAR systems.

Our work addresses these gaps by developing novel tokenization strategies for downsampled sensor data, investigating LLM architectures optimized for resource-constrained environments, and providing comprehensive experimental evaluation across various downsampling levels. Through this research, we aim to bridge the gap between theoretical advances in HAR and their practical deployment in real-world wearable systems.
