\chapter{Related Work}

\section{Human Activity Recognition with Wearable Sensors}

\hspace{2em}Human Activity Recognition (HAR) using wearable sensors emerged as a fundamental research area with applications spanning healthcare monitoring, smart homes, and fitness tracking. Early comprehensive surveys such as ``A Tutorial on Human Activity Recognition Using Wearable Sensors'' by Bulling et al. \cite{Bulling2014HARSurvey} and ``A Survey on Human Activity Recognition using Wearable Sensors'' by Lara et al. \cite{Lara2013Mobile} established the foundational understanding of sensor-based activity recognition, highlighting the challenges of feature extraction from multi-modal sensor data and the importance of temporal modeling for sequential activities.

The evolution of HAR was significantly influenced by advances in deep learning methodologies. Traditional approaches relied heavily on hand-crafted features and shallow machine learning models, but the introduction of deep neural networks revolutionized the field. ``Deep Learning for Sensor-based Activity Recognition: A Survey'' by Wang et al. \cite{Wang2019DeepHAR} provided a comprehensive survey demonstrating how deep learning techniques could automatically learn hierarchical representations from raw sensor data, eliminating the need for manual feature engineering. This paradigm shift enabled more robust and generalizable activity recognition systems.

\subsection{Deep Learning Approaches for HAR}

\hspace{2em}The application of deep learning to HAR yielded remarkable improvements in recognition accuracy and model generalization. ``Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition'' by Yang et al. \cite{Yang2015DeepConvLSTM} pioneered the use of deep convolutional neural networks on multichannel time series data, demonstrating superior performance compared to traditional methods. Their approach effectively captured both spatial relationships between sensor channels and temporal dependencies within activity sequences.

Convolutional Neural Networks (CNNs) proved particularly effective for sensor data processing. ``Real-Time Human Activity Recognition with Deep Neural Networks on Mobile Devices'' by Ignatov et al. \cite{Ignatov2018RealTime} developed real-time HAR systems using CNNs on accelerometer data, achieving impressive accuracy while maintaining computational efficiency suitable for mobile deployment. Similarly, ``Human Activity Recognition with Smartphone Sensors using Deep Learning Neural Networks'' by Ronao and Cho \cite{Ronao2016Human} explored smartphone-based HAR using deep learning, showcasing the potential for ubiquitous activity monitoring through everyday devices.

The integration of recurrent architectures has further enhanced temporal modeling capabilities. ``Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables'' by Hammerla et al. \cite{Hammerla2016Deep} conducted comprehensive comparisons of deep, convolutional, and recurrent models for wearable-based HAR, establishing best practices for different types of sensor configurations and activity categories. Their work highlighted the importance of model architecture selection based on the specific characteristics of the target application.

\section{Transformer and Attention Mechanisms in HAR}

\hspace{2em}The transformer architecture, originally introduced by ``Attention Is All You Need'' (Transformer) by Vaswani et al. \cite{Vaswani2017Attention}, revolutionized sequence modeling across various domains. The self-attention mechanism's ability to capture long-range dependencies and parallel processing capabilities made it particularly attractive for sensor data analysis. Recent works successfully adapted transformer architectures for HAR applications, demonstrating significant improvements over traditional recurrent approaches.

``P2LHAP: Patch-to-Label Human Activity Prediction'' by Li et al. \cite{Li2024P2LHAP} introduced P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackled activity recognition, segmentation, and forecasting simultaneously. Their approach divided sensor data streams into patches served as input tokens, enabling unified processing of multiple HAR tasks within a single model. The patch-based tokenization strategy proved effective for handling variable-length sensor sequences while maintaining computational efficiency.

Recent advances also explored specialised transformer variants for sensor data. ``SAITS: Self-Attention-based Imputation for Time Series'' by Du et al. \cite{Du2023SAITS} proposed SAITS at \textit{AAAI 2023}, introducing dual-masked self-attention blocks that explicitly modeled temporal as well as cross-channel correlations. Their approach surpassed BRITS and Transformer-based baselines on multiple health-care time-series benchmarks while using a fraction of their parameters. ``MoPFormer: Motion-Primitive Transformer'' by Zhang et al. \cite{Zhang2025MoPFormer} proposed MoPFormer, a Motion-Primitive Transformer that enhanced interpretability by tokenizing sensor signals into semantically meaningful motion primitives. This approach addressed the critical challenge of cross-dataset generalization by learning fundamental movement patterns that remained consistent across different data sources.

``ActionFormer'' by Zhao et al. \cite{Zhao2025ActionFormer} adapted the ActionFormer architecture for sensor-based HAR, demonstrating substantial improvements in activity boundary detection and classification accuracy. Their work highlighted the importance of detecting informative channels in multi-sensor configurations, achieving significant performance gains through attention-based channel selection mechanisms.

\section{Large Language Models for Sensor Data}

\hspace{2em}The recent success of Large Language Models (LLMs) inspired researchers to explore their application to sensor data understanding. The foundational work in this area included ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'' by Devlin et al. \cite{Devlin2019BERT} and ``Language Models are Few-Shot Learners'' (GPT-3) by Brown et al. \cite{Brown2020GPT3}, which demonstrated the power of large-scale pre-training for natural language understanding. These advances motivated the development of similar approaches for sensor modalities.

``SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition'' by Li et al. \cite{li2024sensorllm} introduced SensorLLM, a pioneering work that aligns large language models with motion sensors for human activity recognition. Their two-stage framework consists of Sensor-Language Alignment for learning shared representations between sensor data and natural language, followed by Task-Aware Tuning for specific downstream applications. SensorLLM demonstrated remarkable capabilities in zero-shot activity recognition and cross-modal reasoning, establishing a new paradigm for sensor data understanding.

Building upon this foundation, ``SensorLM: A Comprehensive Sensor-Language Foundation Model'' by Zhang et al. \cite{Zhang2025SensorLM} developed SensorLM, a comprehensive sensor-language foundation model that processed over 59.7 million hours of data from more than 103,000 participants. Their hierarchical caption generation pipeline captured statistical, structural, and semantic information from sensor data, enabling sophisticated natural language interactions with wearable sensor systems.

``ADL-LLM: Activity of Daily Living Recognition with Large Language Models'' by Civitarese et al. \cite{Civitarese2024ADLLLM} explored the application of LLMs for Activities of Daily Living recognition in smart home environments. Their ADL-LLM system transformed raw sensor data into textual representations processed by LLMs for zero-shot recognition, demonstrating the potential for deployment in real-world assisted living scenarios.

% Newly added works on zero-shot LLM-based HAR and comprehensive surveys
``HARGPT: A Systematic Study on Zero-Shot Human Activity Recognition with Large Language Models'' by Ji et al. \cite{Ji2024HARGPT} presented \textit{HARGPT}, a systematic study investigating whether off-the-shelf LLMs could serve as zero-shot human activity recognizers without any gradient-based fine-tuning. By feeding raw, downsampled (\SI{10}{\hertz}) IMU sequences directly into GPT-4 and other large models with carefully crafted role-play and chain-of-thought prompts, HARGPT achieved approximately 80\% accuracy on the Capture-24 and HHAR benchmarks. This evidence revealed an emergent ability of LLMs to transform noisy time–series signals into high-level activity semantics purely through prompting, albeit at the expense of large context windows and intensive prompt engineering. In contrast, our two-stage pipeline fine-tuned a compact Chronos encoder--LLaMA architecture on teacher--student enhanced, coarse-grained data, eliminating elaborate prompt design while enabling efficient on-device inference.

Complementing these empirical findings, ``A Dedicated Survey on LLM-Driven Wearable Sensing'' by Ferrara et al. \cite{Ferrara2024LLMSurvey} provided the first dedicated survey on LLM-driven wearable sensing. The paper synthesized early trends, publicly available datasets, evaluation protocols, and open challenges—such as privacy, robustness, and energy efficiency—thereby situating the rapidly growing body of LLM-for-HAR research within a broader interdisciplinary landscape.

\subsection{High-Level Reasoning with LLMs}

\hspace{2em}Beyond basic activity classification, recent research investigated LLMs' capabilities for high-level reasoning over sensor traces. ``LLMSense: Complex Spatiotemporal Reasoning with Large Language Models'' by Ouyang et al. \cite{Ouyang2024LLMSense} developed LLMSense, which harnessed LLMs for complex spatiotemporal reasoning tasks such as dementia diagnosis and occupancy tracking. Their framework demonstrated how LLMs could comprehend long-term sensor patterns and make sophisticated inferences that went beyond simple activity labeling.

``IoTActivity: AI-based System for Complex Activity Tracking in Elderly Care'' by Sun et al. \cite{Sun2024IoTActivity} presented an AI-based system utilizing IoT-enabled ambient sensors and LLMs for complex activity tracking in elderly care. Their approach combined edge device processing with cloud-based LLM reasoning, addressing privacy concerns while maintaining the sophisticated reasoning capabilities required for healthcare applications.

The integration of multiple sensor modalities was explored by ``IoT-LM: A Large Multisensory Language Model for the Internet of Things'' by Mo et al. \cite{Mo2024IoTLM}, who introduced IoT-LM, a large multisensory language model for the Internet of Things. Their MultiIoT dataset encompassed over 1.15 million samples from 12 modalities, enabling comprehensive multisensory understanding and interactive question-answering capabilities.

\section{Energy Efficiency and Resource Constraints}

\hspace{2em}A critical challenge in HAR deployment was the balance between recognition accuracy and computational efficiency, particularly for battery-powered wearable devices. This challenge motivated extensive research into lightweight model architectures and energy-efficient processing strategies.

Traditional approaches to address computational constraints included model compression techniques, quantization, and pruning. However, these methods often required careful tuning and could significantly impact model performance. The development of inherently efficient architectures emerged as a more promising direction.

``Pretrained Actigraphy Transformer (PAT)'' by Ruan et al. \cite{Ruan2024PAT} introduced the Pretrained Actigraphy Transformer (PAT), designed specifically for time-series wearable movement data with emphasis on computational efficiency. Their lightweight transformer architecture achieved state-of-the-art performance in mental health prediction tasks while maintaining interpretability and deployment feasibility on resource-constrained devices.

\subsection{Data Sampling and Downsampling Strategies}

\hspace{2em}The relationship between sensor data resolution and recognition performance presented a fundamental trade-off in wearable HAR systems. Higher sampling rates provided richer temporal information but significantly increased computational and energy costs. Conversely, lower sampling rates extended battery life but could compromise recognition accuracy.

This trade-off became particularly critical for continuous monitoring applications where devices had to operate for extended periods without charging. The challenge of maintaining recognition performance with downsampled sensor data remained largely unexplored in the literature, representing a significant gap that our work addressed.

Current HAR systems typically assumed high-frequency sensor data (50-100 Hz or higher), which was not practical for real-world deployment scenarios where energy efficiency was paramount. The development of effective methods for coarse-grained sensor data processing was essential for enabling practical wearable HAR systems.

\section{Time Series Imputation and Data Enhancement}

\hspace{2em}Missing values and coarse temporal resolutions were pervasive in wearable sensing. Early statistical or RNN-based approaches struggled to recover fine-grained dynamics when large portions of the signal were unobserved. Recent attention-based models achieved substantial progress. ``SAITS: Self-Attention-based Imputation for Time Series'' by Du et al. \cite{Du2023SAITS}, published at \textit{AAAI 2023}, proposed dual-masked self-attention blocks that explicitly modeled temporal as well as cross-channel correlations, surpassing BRITS and Transformer-based baselines on multiple health-care time-series benchmarks while using a fraction of their parameters. Building on this idea, ``Partial Blackout: Self-Attention with Diffusion for Time Series Imputation'' by Islam et al. \cite{Islam2025PartialBlackout} extended self-attention with diffusion processes to tackle the challenging \textit{partial blackout} missing pattern and reported state-of-the-art results in the AAAI~2025 main track. Such advances indicated that self-attention not only excelled at sequence modelling but was also a powerful tool for data reconstruction.

\hspace{2em}Our work leveraged SAITS as a plug-and-play enhancement module for low-frequency wearable streams. In contrast with the above methods, we did not treat imputation as an end in itself; instead, we fed the reconstructed signal into a downstream sensor-language model, demonstrating that high-quality imputation was a crucial enabler for low-power, LLM-based HAR.

\section{Research Gaps and Motivation}

\hspace{2em}Despite significant advances in HAR research, several critical gaps remained. First, while transformer-based approaches showed promise, \textbf{the interplay between advanced imputation models such as SAITS and downstream LLM-based recognisers remained unexplored}. Our work was the first to systematically investigate and exploit this synergy.

Second, the integration of LLMs with sensor data focused primarily on high-frequency, well-sampled data. The effectiveness of LLM-based approaches for downsampled sensor data remained unexplored, despite its practical importance for real-world deployment.

Third, current research lacked comprehensive analysis of the attention patterns and tokenization strategies specifically designed for low-precision sensor data. Understanding how transformer models processed coarse-grained sensor information was crucial for developing effective architectures.

Finally, there was insufficient investigation into the fundamental limits of activity recognition with reduced sensor resolution. Establishing these limits and developing methods to approach them was essential for advancing the field toward practical, energy-efficient HAR systems.

Our work addressed these gaps by developing novel tokenization strategies for downsampled sensor data, investigating LLM architectures optimized for resource-constrained environments, and providing comprehensive experimental evaluation across various downsampling levels. Through this research, we aimed to bridge the gap between theoretical advances in HAR and their practical deployment in real-world wearable systems.
