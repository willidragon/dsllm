Human Activity Recognition (HAR) is a critical research area in wearable sensing technology, with significant implications for health monitoring, sports analysis, and smart living applications. Traditional wearable sensors are often limited by low-precision data and computational constraints, making accurate activity recognition challenging. The rapid advancement of Large Language Models (LLMs) offers new possibilities with their powerful sequence understanding and pattern recognition capabilities.

This thesis proposes an innovative approach that utilizes Large Language Models for human activity recognition from coarse-grained wearable sensors. We design an adaptive framework that transforms coarse-grained time-series data from wearable sensors into representations comprehensible by language models, leveraging the powerful representation learning capabilities of pre-trained language models to enhance activity recognition accuracy.

Our method comprises three core components: (1) a sensor data preprocessing and feature extraction module that converts raw sensor signals into structured sequences; (2) a Large Language Model-based activity pattern learning architecture that utilizes attention mechanisms to capture temporal relationships; and (3) a multi-modal fusion strategy that integrates information from different sensors to improve recognition performance. Experimental results demonstrate that our approach achieves significant performance improvements over traditional methods across multiple public datasets, particularly excelling in handling low-precision and noisy data.

This research provides a new technical pathway for the intelligent development of wearable devices and advances the application innovation of Large Language Models in the Internet of Things and ubiquitous computing domains.

Keywords: Human Activity Recognition; Large Language Models; Wearable Sensors; Coarse-grained Data; Time Series Analysis; Multi-modal Fusion