\chapter{Experiments}

% =====================
% Teacher-Student Model Baseline (Commented Out)
% =====================
%\section{Baseline Methods}
%
%\hspace{2em}To evaluate the effectiveness of our two-stage teacher-student approach, we compare against several baseline methods that represent different paradigms for handling coarse-grained sensor data:
%
%\begin{itemize}
%    \item \textbf{Direct SensorLLM}: The original SensorLLM model \cite{li2024sensorllm} trained directly on low-resolution sensor data without any enhancement preprocessing.
%    
%    \item \textbf{Interpolation + SensorLLM}: A simple baseline that uses linear interpolation to upsample low-resolution data to high-resolution before feeding to SensorLLM.
%    
%    \item \textbf{CNN-LSTM}: A traditional deep learning approach using convolutional layers followed by LSTM for temporal modeling, trained directly on low-resolution data.
%    
%    \item \textbf{Transformer-based HAR}: A pure transformer architecture adapted for sensor data classification, operating on low-resolution inputs.
%\end{itemize}

\section{Baseline Methods}

\hspace{2em}To evaluate the effectiveness of our revised pipeline, we compare the following methods for handling coarse-grained sensor data:

\begin{itemize}
    \item \textbf{Direct SensorLLM}: The original SensorLLM model \cite{li2024sensorllm} trained directly on low-resolution sensor data without any enhancement preprocessing.
    \item \textbf{SAITS + SensorLLM (Ours)}: Our main pipeline, where SAITS imputes/enhances low-resolution data, which is then used for downstream HAR with SensorLLM.
    \item \textbf{LSTM-based Super-Resolution Model + SensorLLM}: A comparison pipeline, where the LSTM-based model enhances low-resolution data, which is then used for downstream HAR with SensorLLM.
\end{itemize}

% =====================
% Teacher-Student Training Configuration (Commented Out)
% =====================
%\subsection{Stage 1: Data Enhancement Training}
%
%\hspace{2em}The teacher-student enhancement model is trained with the following configuration:
%
%\begin{itemize}
%    \item \textbf{Optimizer}: Adam with learning rate $1 \times 10^{-3}$
%    \item \textbf{Batch Size}: 16 samples per batch
%    \item \textbf{Training Epochs}: 30 epochs for teacher, 50 epochs for student
%    \item \textbf{Loss Coefficients}: $\lambda_r = 1.0$, $\lambda_f = 0.5$, $\lambda_s = 1.0$, $\lambda_{spec} = 1.0$
%    \item \textbf{Early Stopping}: Patience of 10 epochs based on validation loss
%    \item \textbf{Data Split}: 70\% training, 15\% validation, 15\% test
%\end{itemize}

\subsection{Stage 1: SAITS Imputation Training}

\hspace{2em}The SAITS model is trained for time series imputation with the following configuration:

\begin{itemize}
    \item \textbf{Optimizer}: AdamW with learning rate $5 \times 10^{-4}$
    \item \textbf{Batch Size}: 256 samples per batch
    \item \textbf{Training Epochs}: Approximately 18 epochs due to early stopping, with no upper limit initially set
    \item \textbf{Early Stopping}: Patience of 30 epochs based on validation loss
    \item \textbf{Data Split}: 70\% training, 15\% validation, 15\% test
\end{itemize}

\subsection{Stage 2: SensorLLM Training}

\hspace{2em}The SensorLLM model is trained on enhanced data with the following setup:

\begin{itemize}
    \item \textbf{Optimizer}: AdamW with cosine learning rate scheduling
    \item \textbf{Learning Rate}: $2 \times 10^{-3}$ with 3\% warmup ratio
    \item \textbf{Batch Size}: 4 per device with gradient accumulation steps of 8
    \item \textbf{Training Epochs}: 8 epochs with early stopping based on F1-macro score
    \item \textbf{Loss Function}: Weighted cross-entropy to handle class imbalance
    \item \textbf{Model Freezing}: LLM and time series encoder frozen, only classification head trainable
\end{itemize}

\begin{table}[h]
\centering
\caption{Complete Training Configuration Summary}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Parameter} & \textbf{Stage 1 (SAITS)} & \textbf{Stage 2 (SensorLLM)} \\ \hline
Learning Rate & $5 \times 10^{-4}$ & $2 \times 10^{-3}$ \\ \hline
Batch Size & 256 & 4 ($\times$8 accumulation) \\ \hline
Epochs & 30 & 8 \\ \hline
Optimizer & AdamW & AdamW \\ \hline
Early Stopping & Validation loss & F1-macro based \\ \hline
\end{tabular}
\end{table}

% =====================
% Teacher-Student Enhancement Quality Metrics (Commented Out)
% =====================
%\subsection{Stage 1: Enhancement Quality Metrics}
%
%\hspace{2em}To evaluate the quality of the enhanced sensor data, we use the following metrics:
%
%\subsubsection{Reconstruction Fidelity}
%\begin{itemize}
%    \item \textbf{Mean Squared Error (MSE)}: $\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{X}^h_i - \hat{\mathbf{X}}^{enhanced}_i\|_2^2$
%    \item \textbf{Mean Absolute Error (MAE)}: $\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{X}^h_i - \hat{\mathbf{X}}^{enhanced}_i\|_1$
%\end{itemize}
%
%\subsubsection{Temporal Alignment}
%\begin{itemize}
%    \item \textbf{Dynamic Time Warping (DTW)}: Measures temporal alignment between enhanced and ground truth sequences
%    \item \textbf{Temporal Correlation}: Pearson correlation coefficient between enhanced and target time series
%\end{itemize}
%
%\subsubsection{Frequency Domain Preservation}
%\begin{itemize}
%    \item \textbf{STFT Magnitude Error}: $\text{STFT-MAE} = \frac{1}{N} \sum_{i=1}^{N} \|\text{STFT}(\mathbf{X}^h_i) - \text{STFT}(\hat{\mathbf{X}}^{enhanced}_i)\|_1$
%    \item \textbf{Power Spectral Density (PSD) Similarity}: Measures preservation of frequency characteristics
%\end{itemize}

\subsection{Stage 1: Imputation Quality Metrics}

\hspace{2em}To evaluate the quality of the imputed sensor data produced by SAITS, we use the following metrics:

\subsubsection{Reconstruction Fidelity}
\begin{itemize}
    \item \textbf{Mean Squared Error (MSE)}: $\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{X}^h_i - \hat{\mathbf{X}}^{imputed}_i\|_2^2$
    \item \textbf{Mean Absolute Error (MAE)}: $\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{X}^h_i - \hat{\mathbf{X}}^{imputed}_i\|_1$
\end{itemize}

\subsubsection{Temporal Alignment}
\begin{itemize}
    \item \textbf{Dynamic Time Warping (DTW)}: Measures temporal alignment between imputed and ground truth sequences
    \item \textbf{Temporal Correlation}: Pearson correlation coefficient between imputed and target time series
\end{itemize}

\subsubsection{Frequency Domain Preservation}
\begin{itemize}
    \item \textbf{STFT Magnitude Error}: $\text{STFT-MAE} = \frac{1}{N} \sum_{i=1}^{N} \|\text{STFT}(\mathbf{X}^h_i) - \text{STFT}(\hat{\mathbf{X}}^{imputed}_i)\|_1$
    \item \textbf{Power Spectral Density (PSD) Similarity}: Measures preservation of frequency characteristics
\end{itemize}

\subsection{Stage 2: HAR Performance Metrics}

\hspace{2em}For the final human activity recognition task, we evaluate performance using standard classification metrics:

\subsubsection{Classification Accuracy}
\[\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}\]

\subsubsection{Macro F1-Score}
\[\text{F1-macro} = \frac{1}{C} \sum_{i=1}^{C} \frac{2 \cdot \text{Precision}_i \cdot \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}\]

where $C$ is the number of activity classes (10 for Capture-24).

\subsubsection{Per-Class Performance}
\begin{itemize}
    \item \textbf{Precision}: $\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}$
    \item \textbf{Recall}: $\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}$
    \item \textbf{F1-Score}: $\text{F1}_i = \frac{2 \cdot \text{Precision}_i \cdot \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}$
\end{itemize}

% =====================
% Teacher-Student Results (Commented Out)
% =====================
%\subsection{Stage 1: Data Enhancement Quality}
%
%\hspace{2em}We first evaluate the quality of the enhanced sensor data produced by our teacher-student framework.
%
%% Placeholder for enhancement quality results table
%\begin{table}[ht]
%    \centering
%    \begin{tabular}{lcccc}
%        \toprule
%        \textbf{Method} & \textbf{MSE} & \textbf{MAE} & \textbf{DTW} & \textbf{STFT-MAE} \\
%        \midrule
%        Linear Interpolation & 0.245 & 0.387 & 12.4 & 0.156 \\
%        Cubic Spline & 0.198 & 0.321 & 10.8 & 0.142 \\
%        Our Teacher-Student & \textbf{0.089} & \textbf{0.201} & \textbf{6.2} & \textbf{0.087} \\
%        \bottomrule
%    \end{tabular}
%    \caption{Data enhancement quality comparison. Lower values indicate better reconstruction fidelity.}
%    \label{tab:enhancement_quality}
%\end{table}

\subsection{Stage 1: Imputation Quality Results}

\hspace{2em}We first evaluate the quality of the imputed sensor data produced by SAITS.

% Placeholder for imputation quality results table
\begin{table}[ht]
    \centering
    \caption{Imputation quality comparison. Lower values indicate better reconstruction fidelity.}
    \label{tab:imputation_quality}
    % Insert imputation quality results here
\end{table}

\subsection{Stage 2: Human Activity Recognition Performance}

\hspace{2em}We evaluate the final HAR performance using imputed data compared to baselines operating on raw low-resolution data.

% Insert final classification result table
\begin{table}[ht]
    \centering
    \caption{F1 Table}
    \label{tab:final_classification_results}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lrrrrr}
    \toprule
    class & Stage2 HiRes & Stage2 LoRes & 2Stage SAITS & 2Stage LSTM & support \\
    \midrule
    bicycling & 0.561 & \underline{0.029} & \textbf{0.337} & 0.000 & 138 \\
    household-chores & 0.456 & \textbf{0.294} & \underline{0.293} & 0.160 & 741 \\
    manual-work & 0.016 & \underline{0.000} & 0.000 & 0.000 & 247 \\
    mixed-activity & 0.135 & \textbf{0.222} & 0.143 & \underline{0.165} & 504 \\
    sitting & 0.828 & \textbf{0.751} & \underline{0.704} & 0.551 & 3121 \\
    sleep & 0.931 & \textbf{0.941} & \underline{0.908} & 0.795 & 3946 \\
    sports & 0.036 & \underline{0.000} & \textbf{0.149} & 0.000 & 46 \\
    standing & 0.030 & \textbf{0.118} & \underline{0.063} & 0.000 & 285 \\
    vehicle & 0.464 & \underline{0.221} & \textbf{0.332} & 0.000 & 231 \\
    walking & 0.326 & \underline{0.202} & \textbf{0.259} & 0.204 & 445 \\
    \bottomrule
    \end{tabular}
    }
\end{table}

The results from the table indicate that the "2Stage SAITS" model consistently outperforms the other models in several key activity categories. Notably, "2Stage SAITS" achieves the highest F1-scores in activities such as bicycling, vehicle, and walking. This suggests that the imputation and enhancement capabilities of the SAITS model are particularly effective in these scenarios, likely due to its ability to better reconstruct and enhance the sensor data for these dynamic activities. The superior performance in these categories highlights the potential of the "2Stage SAITS" approach in improving human activity recognition tasks, especially in environments where data quality and resolution are critical factors.

\clearpage

\subsection{SensorLLM-Only Baseline: Effect of Downsampling Granularity}

\hspace{2em}To further contextualize the impact of input granularity, we evaluate SensorLLM trained directly on downsampled data at four levels: 100DS (baseline), 500DS, 1000DS, and 2000DS. As expected, the baseline (100DS) outperforms models trained on lower granularity data, with performance degrading as the sampling rate decreases.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/CH4_EXPERIMENT/sensorllm_experiment/confusion_matrices_comparison.png}
    \caption{Confusion matrices for SensorLLM trained on different downsampling granularities.}
    \label{fig:sensorllm_confusion_matrices}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/CH4_EXPERIMENT/sensorllm_experiment/f1_score_comparison.png}
    \caption{F1-score comparison for SensorLLM at different downsampling granularities.}
    \label{fig:sensorllm_f1_score}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/CH4_EXPERIMENT/sensorllm_experiment/precision_recall_comparison.png}
    \caption{Precision and recall comparison for SensorLLM at different downsampling granularities.}
    \label{fig:sensorllm_precision_recall}
\end{figure}

% Weighted metrics summary figure
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/CH4_EXPERIMENT/sensorllm_experiment/weighted_metrics_comparison.png}
    \caption{Weighted and macro metrics for SensorLLM-only baseline at different downsampling granularities.}
    \label{fig:sensorllm_weighted_metrics}
\end{figure}

% NOTE: Ensure the following are in your preamble:
% \usepackage{pgfplotstable}
% \usepackage{booktabs}
% \usepackage{siunitx}

% Weighted metrics table (manual, with best values bolded)
\begin{table}[ht]
    \centering
    \caption{SensorLLM-only baseline: weighted and macro metrics for different downsampling granularities. Best value in each column is bolded.}
    \label{tab:sensorllm_downsampling}
    \begin{tabular}{lcccccccc}
        \toprule
        Experiment & Acc. & W-Prec. & W-Rec. & W-F1 & M-Prec. & M-Rec. & M-F1 \\
        \midrule
        Stage2-only 100x  & \textbf{0.738} & \textbf{0.733} & \textbf{0.738} & \textbf{0.722} & \textbf{0.435} & \textbf{0.410} & \textbf{0.378}  \\
        Stage2-only 500x  & 0.721          & 0.711          & 0.721          & 0.709          & 0.354          & 0.337          & 0.312           \\
        Stage2-only 1000x & 0.665          & 0.712          & 0.665          & 0.677          & 0.378          & 0.291          & 0.278           \\
        Stage2-only 2000x & 0.686          & 0.632          & 0.686          & 0.650          & 0.228          & 0.236          & 0.223           \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[H]
    \centering
    \caption{Per-class F1-score for SensorLLM-only baseline at different downsampling granularities. Best value in each activity column is bolded.}
    \label{tab:sensorllm_perclass_f1}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccc}
        \toprule
        Experiment & sleep & sitting & standing & walking & bicycling & vehicle & household-chores & manual-work & sports & mixed-activity \\
        \midrule
        baseline\_100ds  & 0.931 & \textbf{0.828} & 0.030 & \textbf{0.326} & \textbf{0.561} & \textbf{0.464} & \textbf{0.456} & \textbf{0.016} & 0.036 & 0.135 \\
        baseline\_500ds  & \textbf{0.942} & 0.821 & 0.045 & 0.228 & 0.175 & 0.303 & 0.435 & 0.000 & \textbf{0.039} & 0.128 \\
        baseline\_1000ds & \textbf{0.942} & 0.751 & \textbf{0.118} & 0.202 & 0.029 & 0.221 & 0.294 & 0.000 & 0.000 & \textbf{0.222} \\
        baseline\_2000ds & 0.938 & 0.736 & 0.006 & 0.216 & 0.000 & 0.030 & 0.212 & 0.000 & 0.000 & 0.087 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\hspace{2em}These results confirm that SensorLLM's performance is highly sensitive to input resolution, with the highest accuracy and F1 scores achieved at the finest granularity (100DS). As the data becomes more coarse-grained, both weighted and macro metrics decline, underscoring the importance of high-resolution input for robust HAR.