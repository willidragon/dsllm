\chapter{Introduction}

Human Activity Recognition (HAR) using wearable sensors has emerged as a fundamental technology for numerous applications including mobile health monitoring, sports analytics, human-computer interaction, and smart environment systems \cite{Bulling2014HARSurvey, Lara2013Mobile}. The ability to automatically identify and classify daily activities from sensor data enables continuous and unobtrusive monitoring of human behavior, providing valuable insights for healthcare, fitness tracking, and lifestyle analysis \cite{Shoaib2015Survey}.

A critical and underexplored challenge in HAR is the significant drop in recognition performance when sensor data is downsampled or collected at low temporal resolution, a scenario increasingly common in consumer-grade wearables. Reducing the sampling frequency of sensors such as accelerometers and gyroscopes can dramatically extend battery life and operational time for smart devices, making low-grain data collection highly desirable for real-world, long-term deployment. However, this comes at the cost of information loss, increased noise, and reduced discriminative power, which can severely degrade the accuracy of activity recognition models.

Traditional HAR approaches predominantly rely on motion sensors such as accelerometers, gyroscopes, and magnetometers embedded in smartphones or dedicated wearable devices \cite{Chen2021WearableHAR}. While these sensors offer the advantage of being lightweight, power-efficient, and readily available in consumer devices, they often produce coarse-grained data with limited precision compared to specialized research-grade equipment. This limitation poses significant challenges for accurate activity recognition, particularly when distinguishing between subtle variations in movement patterns or activities with similar motion characteristics.

Recent advances in deep learning have revolutionized the field of HAR, with convolutional neural networks (CNNs) and recurrent neural networks (RNNs) showing remarkable performance improvements over traditional machine learning approaches \cite{Wang2019DeepHAR, Yang2015DeepConvLSTM}. Deep learning models have demonstrated their ability to automatically extract meaningful features from raw sensor data, reducing the need for manual feature engineering \cite{Hammerla2016Deep, Ronao2016Human}. However, most existing deep learning approaches for HAR are designed for high-quality sensor data and may not perform optimally when dealing with the noisy, low-resolution signals typical of consumer-grade wearable devices \cite{Ignatov2018RealTime}.

The emergence of Large Language Models (LLMs) and transformer architectures has opened new possibilities for sequence modeling and pattern recognition across diverse domains \cite{Vaswani2017Attention}. Models such as BERT \cite{Devlin2019BERT} and GPT-3 \cite{Brown2020GPT3} have demonstrated exceptional capabilities in understanding complex sequential patterns and contextual relationships. While these models were originally designed for natural language processing tasks, their underlying transformer architecture and attention mechanisms show promise for modeling temporal sequences in other domains, including sensor data analysis.

Recent work has begun to explore the application of LLMs to sensor-based HAR, introducing frameworks for aligning language models with motion sensors for activity recognition. These approaches typically involve specialized alignment stages and task-aware tuning procedures to bridge the gap between natural language processing and sensor-based activity recognition. However, these existing methods face several critical limitations when applied to real-world deployment scenarios involving downsampled, coarse-grained wearable sensors.

The primary challenges include computational requirements and memory footprint that present significant barriers for deployment on resource-constrained wearable devices, where power efficiency and real-time processing are paramount. Additionally, existing approaches have primarily focused on high-quality sensor data from controlled laboratory settings, but have not adequately addressed the unique challenges posed by low-frequency, low-precision sensor signals that are characteristic of consumer-grade wearable devices. Furthermore, the temporal resolution and sampling rates of downsampled wearable sensors may not align optimally with the sequence processing capabilities of language models, necessitating specialized data representation and tokenization strategies.

Our work addresses these challenges through a novel approach that leverages the complementary strengths of advanced time-series processing and language model capabilities. The self-attention mechanism in transformers can effectively capture long-range dependencies in temporal sensor data, potentially identifying subtle patterns that traditional approaches might miss in noisy, low-resolution signals. The pre-training paradigm used in LLMs could enable knowledge transfer from large-scale datasets to smaller, domain-specific HAR datasets, which is particularly valuable when working with limited, downsampled sensor data. Additionally, the contextual understanding capabilities of LLMs may help in disambiguating similar activities by considering broader temporal context, which is crucial when dealing with the reduced discriminative power of low-frequency sensors.

This thesis is specifically motivated by the need to enable accurate and robust HAR on downsampled (low-grain) sensor data, with the ultimate goal of enabling longer battery life and more practical, real-world deployment of smart wearable devices.

A key contribution of this thesis is a \textbf{two-stage pipeline} that first restores high-resolution temporal detail from low-frequency sensor signals using an advanced upsampling framework and then performs activity recognition with a token-efficient downstream model. By separating data enhancement from recognition, we unlock the complementary strengths of state-of-the-art time-series imputation and sensor-language modelling, achieving robust HAR even when the raw input is severely downsampled.

The main contributions of this work include:
\begin{enumerate}
\item A novel \textbf{upsampler + downstream model} framework that integrates cutting-edge time-series imputation with large-scale sensor-language modelling for coarse-grained wearable HAR.
\item An end-to-end processing pipeline and tokenisation strategy that enables efficient fine-tuning of compact models on imputed sensor sequences, facilitating on-device inference without sacrificing accuracy.

\end{enumerate}

The remainder of this thesis is organized as follows: Chapter 2 reviews related work in human activity recognition, deep learning approaches for sensor data analysis, and the application of transformer models to time series data, with particular emphasis on recent developments in LLM-based sensor data processing and the challenges of low-grain data. Chapter 3 presents our proposed methodology, including the downsampled sensor data tokenization approach and the efficient LLM-based HAR architecture. Chapter 4 describes the experimental setup and presents comprehensive evaluation results across multiple datasets and sensor precision levels. Chapter 5 discusses the implications of our findings, limitations of the current approach, and future research directions. Finally, Chapter 6 concludes the thesis and summarizes the key contributions.